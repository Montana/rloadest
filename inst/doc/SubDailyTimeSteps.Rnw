\documentclass{article}
\parskip 6pt
\usepackage[margin=1.25in]{geometry}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}

%\VignetteIndexEntry{Subdaily Time-Step Models}
%\VignetteDepends{rloadest}

\begin{document}
\SweaveOpts{concordance=TRUE}
\raggedright

\title{Subdaily Time-Step Models}

\author{Dave Lorenz}

\maketitle

\begin{abstract}
This example illustrates how to set up and build a rating curve model for a sub-daily time step. Sub-daily time steps can be used for small streams that have peak durations less than one day and have sampling schemes that measure the mean concentration over part or all of the peak flow. This example build a simple model and assumes that the user is familiar with the general steps in build a rating curve model using the functions in \textbf{rloadest}.
\end{abstract}

\tableofcontents

\eject
\section{Introduction}

This example builds on and extends the analysis of Landers and others (2007) and Joiner and others (2014) of whole-water ammonia plus organic nitrogen in the Brushy Fork Creek at Beaver Road near Loganville, Georgia, USGS streamgage 02207400. This example covers the period beginning in water year 2005 through water year 2013.

<<echo=TRUE>>=
# Load the necessary packages and the data
library(rloadest)
library(dataRetrieval)
# Get the QW data
Brushy <- "02207400"
# Parameter code 00665 is whole-water phosphorus
# Parameter code 71123 is mean streamflow for peak flow
# Parameter code 00060 is the daily mean streamflow
# Parameter code 00061 is the measured streamflow
# 71123 will be used as the flow for peak flows and 00061
# for base flow or 00060 if 00061 is missing.
BrushyQW <- importNWISqw("02207400", 
  params=c("00625", "72123", "00060", "00061"),
  begin.date="2004-10-02", end.date="2013-09-30")
# Convert the separate columns of dates and times to a single column
# Uses functions from smwrBase
BrushyQW <- transform(BrushyQW,
  StartDateTime = setTZ(sample_dt + as.timeDay(sample_tm), tzone_cd, force.stz=TRUE),
  EndDateTime = setTZ(sample_end_dt + as.timeDay(sample_end_tm), tzone_cd, force.stz=TRUE))
# A few rows of data:
head(BrushyQW)
# Subset to remove the missing values in Kjeldahl_WW.N.00625
# Note that this works only because we are interested in Kjeldahl_WW.N.00625
BrushyQW <- subset(BrushyQW, !is.na(Kjeldahl_WW.N.00625))
@

\eject
\section{Determine the Time Step}

There is no set method to determine the time step. The approach should determine a typical peak duration. For new sites, with little streamflow record, the user must examine the history of peaks and determine a sampling strategy that can characterize the peak flow. Landers and others (2007) has a brief description of the process. For the Brushy Fork Creek data, with a fairly long history of composite sample collection designed to characterize the peak flow volume, the median duration of the composite sample can be used. The median duration, from the code shown following this paragraph is 352 minutes---the closest even divisor into 24 hours is 6 hours. The code following this paragraph also computes the sample date and time and flow for the regression model.

<<echo=TRUE>>=
# Compute the median peak duration.
with(BrushyQW, median(EndDateTime - StartDateTime, na.rm=T))
# Compute the sample date and time
BrushyQW <- transform(BrushyQW, 
  dateTime=ifelse(is.na(EndDateTime), StartDateTime,
    StartDateTime + (EndDateTime - StartDateTime)/2))
# Need to convert to POSIXct. Note that the original data were
# recorded in standard time only, so the correct time zone is
# "America/Jamaica," which preserves the correct time offset.
BrushyQW <- transform(BrushyQW, dateTime=as.POSIXct(dateTime,
  origin="1970-01-01", tz="America/Jamaica"))
# Now the flow, coalesce is in smwrBase
BrushyQW <- transform(BrushyQW, 
  Flow=ifelse(!is.na(EndDateTime), DischargeMeanStorm_cfs,
  coalesce(InstDischarge_cfs, Discharge_cfs)))
@

\eject
\section{Build the Model}

Surrogate data including turbidity and specific conductance are available for this site, but only as daily values. If those unit or instantaneous values were available, then it would be best to examine models that included surrogate data. The model must be built using streamflow and time. The code following this paragraph builds the 6-hour time step model, using the best-model selection process.

<<echo=TRUE>>=
# Find the "best" predefined model
Brushy.lreg <- selBestModel("Kjeldahl_WW.N.00625", BrushyQW, flow="Flow",
  dates="dateTime", time.step="6 hour", station=Brushy)
print(Brushy.lreg)
@

Model 1 was selected---including only flow. The residual variance is quite large at 0.5673, but the printed diagnostics in general indicate an acceptable model. The diagnostic plots in figures 1-3 also indicate an acceptable model: the fit is reasonably linear and appear to have uniform scatter, there are no issues with serial correlation, and the standardized residuals fall very near the 1:1 line.

<<echo=TRUE>>=
# Set up for graph in vignette
setSweave("graph01", 6, 6)
plot(Brushy.lreg, which = 1, set.up=FALSE)
dev.off()
@

\includegraphics{graph01.pdf}
\paragraph{}

\textbf{Figure 1.} The overall fit.

<<echo=TRUE>>=
# Set up for graph in vignette
setSweave("graph02", 6, 6)
plot(Brushy.lreg, which = 4, set.up=FALSE)
dev.off()
@

\includegraphics{graph02.pdf}
\paragraph{}

\textbf{Figure 2.} The correlogram.

<<echo=TRUE>>=
# Set up for graph in vignette
setSweave("graph03", 6, 6)
plot(Brushy.lreg, which = 5, set.up=FALSE)
dev.off()
@

\includegraphics{graph03.pdf}
\paragraph{}

\textbf{Figure 3.} The q-normal plot.

As a final check on the simple model, compute the jackknife estimates of the parameters. The code following this paragraph demonstrates the use of the \texttt{jackStats} function. The relative bias is small, not indicating a problem with the model.

<<echo=TRUE>>=
# Jackknife statistics
jackStats(Brushy.lreg)
@

\eject
\section{Load Estimation}

Load estimation can easily be done using unit-value data, rather than the user aggregating data by the time step. The code following this paragraph demonstrates a simple load estimation for the month of May, 2013. The data are retrieved, making sure the time zone is set to the local time. The unit-value data are aggregated into 6-hour time steps, taking all of the unit values within the time period; a complete record is not required, but each 6-hour period must have at least one unit value. The Brushy Fork Creek data do not have any unit values for 5:15 through 6:00 on May 22, as indicated in the code below. The user must check for gaps in the record; \texttt{predLoad} will fail if there are gaps greater than the time step.

<<echo=TRUE>>=
# Get the data for May, 2013
BrushyQ <- readNWISuv(Brushy, "00060", startDate="2013-05-01", 
  endDate="2013-05-31", tz="America/New_York")
# Rename the Flow column, must be done manually as renameNWISColumns
# appends _Inst
names(BrushyQ)[5] <- "Flow"
# Show the gap in the record:
BrushyQ[2030:2040,]
# predict the load
predLoad(Brushy.lreg, BrushyQ, by="month")
@

\begin{thebibliography}{9}

\bibitem{Jo}
Joiner, J.K., Aulenbach, B.T., and Landers, M.N., 2014, Watershed characteristics and water-quality trends and loads in 12 watersheds in Gwinnett County, Georgia: U.S. Geological Survey Scientific Investigations Report 2014-5141,
79 p., http://dx.doi.org/10.3133/sir20145141.

\bibitem{Lo}
Landers, M.N., Ankcorn, P.D., and McFadden, K.W., 2007, Watershed effects on streamflow quantity and quality in six watersheds of Gwinnett Count, Georgia: U.S. Geological Survey Scientific Investigations Report 2007-5132, 62 p., Web-only publication at http://pubs.usgs.gov/sir/2007/5132/

\end{thebibliography}

\end{document}
